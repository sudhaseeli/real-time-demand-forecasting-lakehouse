# Real-Time Demand Forecasting Lakehouse Platform

This project showcases an end-to-end **modern data platform** built for real-time analytics and demand forecasting using streaming data, AI/ML pipelines, and a cloud-native lakehouse architecture.

It combines real-time event ingestion, stream processing, batch + streaming ETLs, and machine learning forecasts into a single, production-style data platform designed with DataOps principles in mind.

## ðŸŽ¯ Objective

Build a scalable data platform that can:

- Ingest real-time events (orders, inventory, etc.).
- Process and enrich data using streaming ETL pipelines.
- Store data in a cloud data warehouse / lakehouse (Snowflake, BigQuery, or Databricks).
- Generate live analytics and demand forecasts using AI/ML models.
- Orchestrate and monitor data workflows with Airflow and dbt.
- Follow DataOps best practices around testing, version control, and reproducibility.

## ðŸ—ï¸ High-Level Architecture

1. **Data Ingestion (Real-Time)**  
   Simulated order events are generated by a Python producer and published to a Kafka topic (or written to a local `stream_events/` folder for demo purposes).

2. **Stream Processing & Enrichment**  
   A Spark Structured Streaming job consumes events, cleans and aggregates them, and writes curated data to a bronze/silver layer.

3. **Cloud Data Platform (Lakehouse)**  
   Curated batch + streaming data is loaded into a warehouse/lakehouse (Snowflake, BigQuery, or Databricks).  
   Data is organized into:
   - Bronze (raw events)
   - Silver (cleaned, enriched)
   - Gold (analytics-ready fact/dim models)

4. **Batch & Streaming ETL Orchestration**  
   Apache Airflow orchestrates:
   - Batch ingestion jobs
   - Backfills of historical data
   - dbt model runs
   - ML training and forecast generation

5. **Transformations & Modeling (dbt)**  
   dbt is used for:
   - Staging models (stg_*)
   - Dimension tables (dim_*)
   - Fact tables (fact_*)
   with tests and documentation to ensure data quality.

6. **AI/ML Demand Forecasting**  
   A Python ML pipeline trains a simple demand forecasting model (e.g., using Prophet or scikit-learnâ€™s regression models) and writes predictions back into the warehouse.

7. **Analytics & Consumption**  
   Gold-layer tables (including forecasts) can be connected to BI tools like Power BI, Tableau, or Looker.

## ðŸ§° Tech Stack

- Python
- Apache Kafka (simulated via local files if Kafka is not available)
- Apache Spark / PySpark
- Apache Airflow
- dbt (Data Build Tool)
- Snowflake / BigQuery / Databricks (warehouse/lakehouse)
- scikit-learn (or similar) for ML
- Git for version control

## ðŸ“‚ Project Structure

```bash
real-time-demand-forecasting-lakehouse/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ warehouse_config.yml
â”‚   â””â”€â”€ streaming_config.yml
â”œâ”€â”€ streaming/
â”‚   â”œâ”€â”€ kafka_producers/
â”‚   â”‚   â””â”€â”€ simulate_orders.py
â”‚   â””â”€â”€ stream_events/
â”œâ”€â”€ spark_jobs/
â”‚   â”œâ”€â”€ batch_ingestion.py
â”‚   â””â”€â”€ feature_engineering.py
â”œâ”€â”€ airflow_dags/
â”‚   â””â”€â”€ demand_forecasting_pipeline_dag.py
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/
â”‚       â”‚   â””â”€â”€ stg_orders.sql
â”‚       â””â”€â”€ marts/
â”‚           â””â”€â”€ fact_demand_forecast.sql
â””â”€â”€ ml/
    â”œâ”€â”€ train_model.py
    â””â”€â”€ generate_forecasts.py
```

> Note: This repo is designed to be **portfolio-friendly**: code is written to be readable and educational, and it can be adapted to real infrastructure with minimal changes.

## ðŸš€ Quick Start (Local Demo Mode)

1. Create and activate a virtual environment.
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Generate sample streaming events:
   ```bash
   python streaming/kafka_producers/simulate_orders.py
   ```
4. Run batch ingestion (simulated warehouse load):
   ```bash
   python spark_jobs/batch_ingestion.py
   ```
5. Run feature engineering and model training:
   ```bash
   python ml/train_model.py
   python ml/generate_forecasts.py
   ```

For a full production-style setup, you can integrate:
- Real Kafka and Spark streaming clusters
- A real Airflow instance
- A real warehouse like Snowflake/BigQuery/Databricks



This project demonstrates:

- Real-time style event ingestion and streaming ETL logic.
- Experience with Airflow, Spark, dbt, and cloud warehouses/lakehouses.
- End-to-end ML demand forecasting pipeline design.
- DataOps mindset: structure, modularity, and clear documentation.


